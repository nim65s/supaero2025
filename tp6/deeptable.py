"""
Example of Q-table learning with a simple discretized 1-pendulum environment
using a linear Q network.
"""

import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
from myenvs import EnvMountainCarFullyDiscrete
import torch
import torch.nn as nn
import torch.nn.functional as F

### --- Random seed
RANDOM_SEED = 1188  # int((time.time()%10)*1000)
print("Seed = %d" % RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

### --- Hyper paramaters
NEPISODES = 20000  # Number of training episodes
NSTEPS = 50  # Max episode length
LEARNING_RATE = 0.6  # Step length in optimizer
DECAY_RATE = 0.99  # Discount factor

### --- Environment

env = EnvMountainCarFullyDiscrete()#render_mode='human')

### --- Q-value networks

class QValueNetwork(nn.Module):
    def __init__(self, env, learning_rate=LEARNING_RATE):
        assert isinstance(env.observation_space,gym.spaces.discrete.Discrete)
        assert isinstance(env.action_space,gym.spaces.discrete.Discrete)

        super(QValueNetwork, self).__init__()
        
        # Linear layer from input size NX to output size NU
        self.fc = nn.Linear(env.observation_space.n,env.action_space.n)
        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)
    
    def forward(self, x):
        qvalue = self.fc(x)
        return qvalue
    
    def predict_action(self, x, noise=0):
        qvalue = self.forward(x)
        if noise is not 0:
            qvalue += torch.randn(qvalue.shape) * noise
        u = torch.argmax(qvalue,dim=1)
        return u
    
    def update(self, x, qref):
        self.optimizer.zero_grad()
        qvalue = self.forward(x)
        loss = F.mse_loss(qvalue, qref)
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def plot(self):
        
        # ### Plot Q-Table as value function pos-vs-vel
        def indexes_to_continuous_states(indexes):
            return [ env.undiscretize_state(env.index_to_discrete_state(s))
                     for s in indexes if env.is_index_in_range(s) ]
        Q = self.fc.weight.T
        fig,axs = plt.subplots(1,2,figsize=(10,5))
        qvs = indexes_to_continuous_states(range(env.observation_space.n))
        values = [ float(torch.max(Q[s,:])) for s in range(env.observation_space.n)
                   if env.is_index_in_range(s) ]
        vplot = axs[0].scatter([qv[0] for qv in qvs],[qv[1] for qv in qvs],c=values)
        plt.colorbar(vplot,ax=axs[0])
        policies = [ int(torch.argmax(Q[s,:])) for s in range(env.observation_space.n)
                     if env.is_index_in_range(s) ]
        axs[1].scatter([qv[0] for qv in qvs],[qv[1] for qv in qvs],c=policies)



    
qvalue = QValueNetwork(env)

def one_hot(ix, env):
    """Return a one-hot encoded tensor.
    
    - ix: index or batch of indices
    - n: number of classes (size of the one-hot vector)
    """
    ix = torch.tensor(ix).long()
    if ix.dim() == 0:  # If a single index, add batch dimension
        ix = ix.unsqueeze(0)
    return F.one_hot(ix,num_classes=env.observation_space.n).to(torch.float32)

### --- History of search
h_rwd = []  # Learning history (for plot).

### --- Training
for episode in range(1, NEPISODES):
    x, _ = env.reset()
    rsum = 0.0

    for step in range(NSTEPS - 1):

        u = int(qvalue.predict_action(one_hot(x,env),noise=1/episode))  # ... with noise
        x2, reward, done, trunc, info = env.step(u)

        # Compute reference Q-value at state x respecting HJB
        # Q2 = sess.run(qvalue.qvalue, feed_dict={qvalue.x: onhot(x2)})
        qnext = qvalue(one_hot(x2,env))
        qref = qvalue(one_hot(x,env))
        qref[0, u] = reward + DECAY_RATE * torch.max(qnext)

        # Update Q-table to better fit HJB
        #sess.run(qvalue.optim, feed_dict={qvalue.x: onehot(x), qvalue.qref: Qref})
        qvalue.update(x=one_hot(x,env),qref=qref)
        
        rsum += reward
        x = x2
        if done or trunc: break

    h_rwd.append(rsum)
    if not episode % 20:
        print("Episode #%d done with %d sucess" % (episode, sum(h_rwd[-20:])))

### DISPLAY

# ### Render a trial

def rendertrial(env,maxiter=100):
    """Roll-out from random state using greedy policy."""
    s,_ = env.reset()
    traj = [s]
    for i in range(maxiter):
        a = int(qvalue.predict_action(one_hot(s,env)))
        s, r, done, trunc, _ = env.step(a)
        traj.append(s)
        if done or trunc: break
    return traj

envrender = EnvMountainCarFullyDiscrete(render_mode = 'human')
traj = rendertrial(envrender,10)

plt.ion()

# ### Plot the learning curve
print("Total rate of success: %.3f" % (sum(h_rwd) / NEPISODES))
plt.plot(np.cumsum(h_rwd) / range(1, len(h_rwd)+1))

# ### Plot Q-Table as value function pos-vs-vel
qvalue.plot()
